---
title: memories embed
description: Generate embeddings for semantic search.
---

```bash
memories embed [options]
```

Generate vector embeddings for memories that don't have them yet. These embeddings power semantic search (`memories search --semantic`).

## How it works

The command:
1. Finds all memories without embeddings
2. Downloads the embedding model if needed (~50MB for default model)
3. Generates vector embeddings for each memory
4. Stores embeddings in the local database

Everything runs locally — no API calls, no data leaves your machine.

## Options

| Option | Description |
|--------|-------------|
| `--all` | Regenerate embeddings for all memories, not just missing ones |
| `--dry-run` | Show what would be embedded without doing it |

## Examples

Generate embeddings for memories without them:

```bash
memories embed
```

Output:
```
Embedding 42 memories...

Model: all-MiniLM-L6-v2 (384d, fast)
First run downloads the model. Subsequent runs are faster.

✓ Embedded 42 memories
```

Preview what would be embedded:

```bash
memories embed --dry-run
```

Regenerate all embeddings (after switching models):

```bash
memories embed --all
```

## When to run

### Automatic

Memories added with `memories add` automatically get embeddings. You don't need to run `memories embed` for new memories.

### Manual

Run `memories embed` when:

1. **After importing memories** — If you used `memories import` or `memories ingest`
2. **After syncing from cloud** — If memories were synced without embeddings
3. **After switching models** — Run `memories embed --all` to regenerate
4. **First-time semantic search** — If you have existing memories and want to use `--semantic`

## Embedding models

The default model is `all-MiniLM-L6-v2`:

- **Size**: ~50MB download
- **Dimensions**: 384-dimensional vectors
- **Speed**: Fast (~10 seconds for 100 memories)
- **Quality**: Good for most use cases

You can switch to higher-quality models for better semantic accuracy:

```bash
# View available models
memories config model

# Switch to a better model
memories config model gte-base
memories embed --all
```

See [Embedding Models](/docs/concepts/embedding-models) for detailed comparison and recommendations.

## Storage

Embedding storage depends on the model:

| Model | Dimensions | Storage per memory |
|-------|------------|-------------------|
| `all-MiniLM-L6-v2` | 384 | ~1.5 KB |
| `gte-base` | 768 | ~3 KB |
| `gte-large` | 1024 | ~4 KB |

## Troubleshooting

### Model download fails

If the model download fails, check your internet connection and try again:

```bash
memories embed
```

The model is cached at `~/.cache/memories/models/`, so partial downloads will resume.

### Embeddings not working

If semantic search isn't returning results:

1. Check that embeddings exist: `memories stats`
2. Regenerate if needed: `memories embed --all`
3. Try a different model: `memories config model gte-base`

### Large database

If your database is getting large, embeddings are the main contributor. Consider:

1. Using a smaller model (`all-MiniLM-L6-v2` vs `gte-large`)
2. Using keyword search for simple queries
3. Only generating embeddings for important memories
